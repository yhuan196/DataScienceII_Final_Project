---
title: "Data science final guideline"
author: "Yi Huang, Yuchen Zhang, Shun Xie"
date: "2023-04-28"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 2
  fontsize: 10pt
  geometry: margin=0.5in
  header-includes:
    -\usepackage{fancyhdr}
    -\usepackage{lipsum}
    -\pagestyle{fancy}
    -\fancyhead[R]{\thepage}
    -\fancypagestyle{plain}{\pagestyle{fancy}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.align = "center",
                      fig.width = 8, 
                      fig.height = 6,
                      out.width = "90%")
```

# Final Project


## Deliverables and deadlines
Due: May 10 by 11:59pm\
Each team is required to submit one report.


## Dataset

Please merge the midterm project datasets from two team members, and keep only the unique observations. If there are three team members, you can decide which two datasets to merge. 


## Background

To gain a better understanding of the factors that predict recovery time from COVID-19 illness, a study was designed to combine three existing cohort studies that have been tracking participants for several years. The study collects recovery information through questionnaires and medical records, and leverages existing data on personal characteristics prior to the pandemic. The ultimate goal is to develop a prediction model for recovery time and identify important risk factors for long recovery time.


## Data

recovery.RData 

The dataset in "recovery.RData" consists of 10000 participants. In your analysis, please draw a random sample of 2000 participants using the following R code:

set.seed([last four digits of your UNI]) 

dat <- dat[sample(1:10000, 2000),]

The resulting dat object will contain a random sample of 2000 participants that you can use for your analysis.


## Here is a description of each variable:

Variable Description

- ID (id) :Participant ID

- Gender (gender): 1 = Male, 0 = Female

- Race/ethnicity (race): 1 = White, 2 = Asian, 3 = Black, 4 = Hispanic

- Smoking (smoking): Smoking status; 0 = Never smoked, 1 = Former smoker, 2 = Current smoker

- Height (height): Height (in centimeters)

- Weight (weight): Weight (in kilograms)

- BMI (bmi): Body Mass Index; BMI = weight (in kilograms) / height (in meters) squared

- Hypertension (hypertension): 0 = No, 1 = Yes

- Diabetes (diabetes): 0 = No, 1 = Yes

- Systolic blood pressure (SBP): Systolic blood pressure (in mm/Hg)

- LDL cholesterol (LDL): LDL (low-density lipoprotein) cholesterol (in mg/dL)

- Vaccination status at the time of infection (vaccine): 0 = Not vaccinated, 1 = Vaccinated

- Severity of COVID-19 infection (severity): 0 = Not severe, 1= Severe

- Study (study): The study (A/B/C) that the participant belongs to

- Time to recovery (tt_recovery_time): Time from COVID-19 infection to recovery in days
 

## Report

Your report should be no more than five pages, excluding figures and tables. The total number of figures and tables should not exceed 8. You can submit an Appendix if you would like to include more details for modeling tuning.

For the primary analysis on time to recovery, you may revise your midterm report and incorporate new findings from the methods introduced in the second half of the semester.

As a secondary analysis, please consider time to recovery as a binary outcome (>30 days vs. <= 30 days) and develop a prediction model for this binary outcome.

 
## Note

Similar to the midterm project report, your report should include the following sections: exploratory analysis and data visualization, model training, results, and conclusions. 


```{r, message=FALSE, warning=FALSE}
library(caret)
library(corrplot)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(ISLR)
library(glmnet)
library(mgcv)
library(nlme)
library(earth)
library(vip)
```

## Introduction & data manipulation
```{r}
#load data
load("data/recovery.RData")

#get data from 2 team members uni
set.seed(3554)
dat1 <- dat[sample(1:10000, 2000),]
set.seed(4437)
dat2 <- dat[sample(1:10000, 2000),]

#combine the data and discard duplicates
dat_temp <- rbind(dat1, dat2)
dat <- dat_temp[!duplicated(dat_temp$id),]

#dat is now the final data and it has the length as following:
length(dat$id)

```


```{r}
# data manipulation
data = 
  dat %>% 
  mutate(study = factor(dat$study),
         gender = factor(dat$gender),
         hypertension = factor(dat$hypertension),
         diabetes = factor(dat$diabetes),
         vaccine = factor(dat$vaccine),
         severity = factor(dat$severity)) %>% 
  dplyr::select(-id)
# save(data, file = "data/covid_recovery.RData")
head(data)

 
#Split data into 70-300, using the third member's uni
set.seed(2337)
indexTrain <- createDataPartition(y = data$recovery_time, p = 0.7, list = FALSE)

# training data
train_data <- data[indexTrain,]
# matrix of predictors
train_x <- model.matrix(recovery_time~.,train_data)[,-1]
# vector of response
train_y <- train_data$recovery_time

# test data
test_data <- data[-indexTrain,]
# matrix of predictors
test_x <- model.matrix(recovery_time~.,test_data)[,-1]
# vector of response
test_y <- test_data$recovery_time
```



## Exploratory Data Analysis on train data

```{r}
#continuous
ggplot(train_data, aes(x = age))+geom_density()+labs(title="Distribution for age")
ggplot(train_data, aes(x = height))+geom_density()+labs(title="Distribution for height")
ggplot(train_data, aes(x = weight))+geom_density()+labs(title="Distribution for weight")
ggplot(train_data, aes(x = bmi))+geom_density()+labs(title="Distribution for bmi")
ggplot(train_data, aes(x = SBP))+geom_density()+labs(title="Distribution for SBP")
ggplot(train_data, aes(x = LDL))+geom_density()+labs(title="Distribution for LDL")
ggplot(train_data, aes(x = recovery_time))+geom_density()+labs(title="Distribution for recovery_time")


#discrete
ggplot(train_data) + geom_bar(aes(x=gender))+labs(title="Bar plot for different gender")
ggplot(train_data) + geom_bar(aes(x=race))+labs(title="Bar plot for different race")
ggplot(train_data) + geom_bar(aes(x=smoking))+labs(title="Bar plot for different smoking")
ggplot(train_data) + geom_bar(aes(x=hypertension))+labs(title="Bar plot for different hypertension")
ggplot(train_data) + geom_bar(aes(x=diabetes))+labs(title="Bar plot for different diabetes")
ggplot(train_data) + geom_bar(aes(x=vaccine))+labs(title="Bar plot for different vaccine")
ggplot(train_data) + geom_bar(aes(x=severity))+labs(title="Bar plot for different severity")
ggplot(train_data) + geom_bar(aes(x=study))+labs(title="Bar plot for different study")


num_df <- 
  train_data %>% 
  dplyr::select(age, height, weight, bmi, SBP, LDL, recovery_time) 


# calulate the correlations
res <- cor(num_df, use="complete.obs")

corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

```

```{r}

ggplot(train_data, aes(x = age, y = recovery_time))+geom_point()+geom_smooth(method = 'gam', se = TRUE, color = 'red')+labs(title="Recovery time against age")
ggplot(train_data, aes(x = height, y = recovery_time))+geom_point()+geom_smooth(method = 'gam', se = TRUE, color = 'red')+labs(title="Recovery time against height")
ggplot(train_data, aes(x = weight, y = recovery_time))+geom_point()+geom_smooth(method = 'gam', se = TRUE, color = 'red')+labs(title="Recovery time against weight")
ggplot(train_data, aes(x = bmi, y = recovery_time))+geom_point()+geom_smooth(method = 'gam', se = TRUE, color = 'red')+labs(title="Recovery time against bmi")
ggplot(train_data, aes(x = SBP, y = recovery_time))+geom_point()+geom_smooth(method = 'gam', se = TRUE, color = 'red')+labs(title="Recovery time against SBP")
ggplot(train_data, aes(x = LDL, y = recovery_time))+geom_point()+geom_smooth(method = 'gam', se = TRUE, color = 'red')+labs(title="Recovery time against LDL")

```



## Model training


# Resampling method
```{r}
# 10-fold cv
ctrl <- trainControl(method = "cv",
                     number = 10)
```

### primary analysis (Regression)

* **Linear regression**
* **Partial least squares (PLS)**
* **Generalized Additive Model (GAM)**
* **Multivariate Adaptive Regression Splines (MARS)**
* **Elastic net**
* **Boosting**
* **Random forest**

#### Linear regression
```{r}
## fit linear model on train data
set.seed(8)
linear_model <- train(train_x,
                      train_y,
                      method = "lm", 
                      trControl = ctrl)
summary(linear_model)

# view performance on the test set (RMSE)
lm_test_pred <- predict(linear_model, newdata = test_x) # test dataset
lm_test_rmse <- sqrt(mean((lm_test_pred - test_y)^2))
sprintf("test error for lm is: %.3f",lm_test_rmse)
```


### Enet

```{r}
#tuning glmnet alpha 0 and 0.05 have large RMSE. So alpha start from 0.1
set.seed(8)
enet.fit.min <- train(train_x, train_y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0.1, 1, length = 21), 
                                         lambda = exp(seq(-10, -5, length = 100))),
                  trControl = ctrl)
enet.fit.min$bestTune
plot(enet.fit.min)


# view performance on the test set (RMSE)
enet_test_pred <- predict(enet.fit.min, newdata = test_x) # test dataset
enet_test_rmse <- sqrt(mean((enet_test_pred - test_y)^2))
sprintf("test error for enet is: %.3f",enet_test_rmse)

```

#### PLS
```{r}
set.seed(8)
pls_model <- train(train_x,
                   train_y,
                   method = "pls",
                   tuneGrid = data.frame(ncomp = 1:16),
                   trControl = ctrl,
                   preProcess = c("center", "scale"))
ggplot(pls_model, highlight = TRUE)
ggsave(file = "image/pls_number_of_component.png", width = 10, height = 7)

# view performance on the test set (RMSE)
pls_test_pred <- predict(pls_model, newdata = test_x) # test dataset
pls_test_rmse <- sqrt(mean((pls_test_pred - test_y)^2))
sprintf("test error for pls is: %.3f",pls_test_rmse)

```






#### Modeling Strategy


* **Penalized logistic regression** 
* **Generalized additive model (GAM)** 
* **Multivariate adaptive regression splines (MARS)** 
* **Linear discriminant analysis (LDA)**
* **Random Forest**
* **Boosting**



#### Optimal Tuning Parameters

#### Resampling Results and Model Selection

### secondary analysis (Classification)

#### Data Manipulation


```{r}
#consider time to recovery as a binary outcome (>30 days vs. <= 30 days) and develop a prediction model for this binary outcome.
train_class = 
  train_data %>% 
  mutate(recovery_time = ifelse(recovery_time>30, "long", "short"))

test_class = 
  test_data %>% 
  mutate(recovery_time = ifelse(recovery_time>30, "long", "short"))

# matrix of predictors
x_train_class <- model.matrix(recovery_time~.,train_class)[,-1]
# vector of response
y_train_class <- train_class$recovery_time


# matrix of predictors
x_test_class <- model.matrix(recovery_time~.,test_class)[,-1]
# vector of response
y_test_class <- test_class$recovery_time


# ctrl for class
ctrl_class <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
```





#### Modeling Strategy


* **Penalized logistic regression** 
```{r}
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-12, -7, length = 50)))
set.seed(8)
model.glmn <- train(x = x_train_class,
                    y = y_train_class,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl_class)

model.glmn$bestTune

myCol<- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(model.glmn, par.settings = myPar, xTrans = function(x) log(x))


# view performance on the test set (accuracy)
glmn_class_test_pred <- predict(model.glmn, newdata=x_test_class) # test dataset
glmn_class_test_acc <- sum(glmn_class_test_pred==y_test_class)/length(y_test_class)
sprintf("test error for penalized logistic is: %.3f",1-glmn_class_test_acc)


```

Since the boundary value for $\lambda$ is exp(-10)=4.54e-5 and exp(-5)=0.007, and the optimal lambda is not on the boundary. So local minimum is achieved.


* **Penalized logistic regression** 
* **Generalized additive model (GAM)** 
* **Multivariate adaptive regression splines (MARS)** 
* **Linear discriminant analysis (LDA)**
* **Classification and Regression Tree (CART)**
* **Random Forest**
* **Boosting**
* **Support Vector Machine (SVM with Linear and Radial Kernels)**



#### Penalized logistic regression
```{r}
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-12, -7, length = 50)))
set.seed(8)
model.glmn <- train(x = x_train_class,
                    y = y_train_class,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl_class)

model.glmn$bestTune

myCol<- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(model.glmn, par.settings = myPar, xTrans = function(x) log(x))


# view performance on the test set (accuracy)
glmn_class_test_pred <- predict(model.glmn, newdata=x_test_class) # test dataset
glmn_class_test_acc <- sum(glmn_class_test_pred==y_test_class)/length(y_test_class)
sprintf("test error for penalized logistic is: %.3f",1-glmn_class_test_acc)


```










#### linear SVM

```{r}
# kernlab
set.seed(8)
svml.fit <- train(recovery_time ~ . , 
                  data = train_class, 
                  method = "svmLinear",
                  tuneGrid = data.frame(C = exp(seq(-4,5,len=10))),
                  trControl = ctrl)

plot(svml.fit, highlight = TRUE, xTrans = log)

#train error
train.pred.svml <- predict(svml.fit, newdata = train_class)
train.error.svml = 1-sum(train_class$recovery_time == train.pred.svml)/length(train_data$recovery_time)
sprintf("The train error for Support vector classifier is %.3f", train.error.svml)

#test error
test.pred.svml <- predict(svml.fit, newdata = test_class)
test.error.svml = 1-sum(test_class$recovery_time == test.pred.svml)/length(test_class$recovery_time)
sprintf("The test error for Support vector classifier is %.3f", test.error.svml)
```

#### Optimal Tuning Parameters

#### Resampling Results and Model Selection

## Results

### primary analysis


#### Interpretion   

#### Variable Importance

#### model's training/test performance

### secondary analysis

#### Interpretion   

#### Variable Importance

#### model's training/test performance

## Conclusion

## Appendix

